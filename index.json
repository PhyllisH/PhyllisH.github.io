
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    [{"authors":null,"categories":null,"content":"I am a postdoctoral fellow at University of Michigan in the Department of Robotics, working with Prof. Maani Ghaffari, Prof. Henry Liu, and Prof. Ram Vasudevan. I obtained my Ph. D. degree in Shanghai Jiao Tong Universiy under the supervision of Prof. Siheng Chen. My primary research interest is robotic perception and navigation, and multi-agent collaboration.\nI am actively looking for jobs and collaboration opportunities. Do not hesitate to drop me an email if you are interested :)\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"I am a postdoctoral fellow at University of Michigan in the Department of Robotics, working with Prof. Maani Ghaffari, Prof. Henry Liu, and Prof. Ram Vasudevan. I obtained my Ph. D.","tags":null,"title":"Yue Hu","type":"authors"},{"authors":["Zibo Zhou","Yue Hu","Lingkai Zhang","Zonglin Li","Siheng Chen"],"categories":[],"content":"","date":1758153600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1666084476,"objectID":"2b43c02097a445d469ddc040735d2418","permalink":"https://yuehu.github.io/publication/zhou-beleifmap-2025/","publishdate":"2025-09-18T00:00:00Z","relpermalink":"/publication/zhou-beleifmap-2025/","section":"publication","summary":"","tags":[],"title":"BeliefMapNav: 3D voxel-based belief map for zero-shot object navigation [NeurIPS]","type":"publication"},{"authors":["Yue Hu","Junzhe Wu","Ruihan Xu","Hang Liu","Avery Xi","Henry X Liu","Ram Vasudevan","Maani Ghaffari"],"categories":[],"content":"","date":1758153600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1666084476,"objectID":"69f9599a0c9f7fdbadb5f86205ffec07","permalink":"https://yuehu.github.io/publication/hu-sgimaginenav-2025/","publishdate":"2025-09-18T00:00:00Z","relpermalink":"/publication/hu-sgimaginenav-2025/","section":"publication","summary":"","tags":[],"title":"Imaginative world modeling with scene graphs for embodied agent navigation [ICCV Demo Track]","type":"publication"},{"authors":["Congzhang Shao","Quan Yuan","Guiyang Luo","Yue Hu","Liu Yilin","Danni Wang","Rui Pan","Bo Chen","Jinglin Li"],"categories":[],"content":"","date":1758153600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1666084476,"objectID":"fa02536b3f750b14dc2d4040b70a7a90","permalink":"https://yuehu.github.io/publication/shao-negocolla-2025/","publishdate":"2025-09-18T00:00:00Z","relpermalink":"/publication/shao-negocolla-2025/","section":"publication","summary":"","tags":[],"title":"NegoCollab: A Common Representation Negotiation Approach for Heterogeneous Collaborative Perception [NeurIPS]","type":"publication"},{"authors":["Yue Hu","Yuzhu Cai","Yaxin Du","Xinyu Zhu","Xiangrui Liu","Zijie Yu","Yuchen Hou","Shuo Tang","Siheng Chen"],"categories":[],"content":"","date":1740787200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1666084476,"objectID":"3f8c11a40a0f8c9b918ce67a1fa7685e","permalink":"https://yuehu.github.io/publication/hu-evomac-2025/","publishdate":"2025-03-01T00:00:00Z","relpermalink":"/publication/hu-evomac-2025/","section":"publication","summary":"","tags":[],"title":"Self-Evolving Multi-Agent Collaboration Networks for Software Development [ICLR 2025]","type":"publication"},{"authors":["Yifan Lu","Yue Hu","Yiqi Zhong","Dequan Wang","Siheng Chen","Yanfeng Wang"],"categories":[],"content":"","date":1704067200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1666084476,"objectID":"2a102b089567b4b315564e82331190a0","permalink":"https://yuehu.github.io/publication/lu-heal-2024/","publishdate":"2024-01-01T00:00:00Z","relpermalink":"/publication/lu-heal-2024/","section":"publication","summary":"","tags":[],"title":"An Extensible Framework for Open Heterogeneous Collaborative Perception [ICLR2024]","type":"publication"},{"authors":["Yue Hu","Juntong Peng","Sifei Liu","Junhao Ge","Si Liu","Siheng Chen"],"categories":[],"content":"","date":1704067200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1666084476,"objectID":"1ca231ce61cfbe9ba9114409eac1ac62","permalink":"https://yuehu.github.io/publication/hu-codefilling-2024/","publishdate":"2024-01-01T00:00:00Z","relpermalink":"/publication/hu-codefilling-2024/","section":"publication","summary":"","tags":[],"title":"Communication-Efficient Collaborative Perception via Information Filling with Codebook [CVPR2024]","type":"publication"},{"authors":["Yue Hu","Xianghe Pang","Xiaoqi Qin","Yonina C Eldar","Siheng Chen","Ping Zhang","Wenjun Zhang"],"categories":[],"content":"","date":1704067200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1666084476,"objectID":"3a9afb4a7916a98257e88ae9ddb12503","permalink":"https://yuehu.github.io/publication/hu-pragcomm-2024/","publishdate":"2024-01-01T00:00:00Z","relpermalink":"/publication/hu-pragcomm-2024/","section":"publication","summary":"","tags":[],"title":"Pragmatic Communication in Multi-Agent Collaborative Perception [TPAMI Submitted]","type":"publication"},{"authors":["Genjia Liu","Yue Hu","Chenxin Xu","Weibo Mao","Junhao Ge","Zhengxiang Huang","Yifan Lu","Yinda Xu","Junkai Xia","Yafei Wang","Siheng Chen"],"categories":[],"content":"","date":1704067200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1666084476,"objectID":"cc860949b5a723a17a75ae656269f809","permalink":"https://yuehu.github.io/publication/liu-v2xverse-2025/","publishdate":"2024-01-01T00:00:00Z","relpermalink":"/publication/liu-v2xverse-2025/","section":"publication","summary":"","tags":[],"title":"Towards Collaborative Autonomous Driving Simulation Platform and End-to-End System [TPAMI]","type":"publication"},{"authors":["Yue Hu","Yifan Lu","Runsheng Xu","Weidi Xie","Siheng Chen","Yanfeng Wang"],"categories":[],"content":"","date":1687392000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1687392000,"objectID":"aeb28c3ec563381a968a81ef83ffe722","permalink":"https://yuehu.github.io/publication/hu-coca3d-2023/","publishdate":"2023-06-22T00:00:00Z","relpermalink":"/publication/hu-coca3d-2023/","section":"publication","summary":"","tags":[],"title":"Collaboration Helps Camera Overtake LiDAR in 3D Detection [CVPR2023]","type":"publication"},{"authors":["Yue Hu","Shaoheng Fang","Weidi Xie","Siheng Chen"],"categories":[],"content":"","date":1673827200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1674008541,"objectID":"aae705ada34e58c7eb5184dfa538ec35","permalink":"https://yuehu.github.io/publication/hu-aerial-2023/","publishdate":"2023-01-18T02:22:21.068117Z","relpermalink":"/publication/hu-aerial-2023/","section":"publication","summary":"","tags":[],"title":"Aerial monocular 3d object detection [RAL2023]","type":"publication"},{"authors":["Sizhe Wei","Yuxi Wei","Yue Hu","Yifan Lu","Yiqi Zhong","Siheng Chen","Ya Zhang"],"categories":[],"content":"","date":1672531200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1666084476,"objectID":"b4a84468787208fae46c174655944f3a","permalink":"https://yuehu.github.io/publication/wei-cobevflow-2023/","publishdate":"2023-01-01T00:00:00Z","relpermalink":"/publication/wei-cobevflow-2023/","section":"publication","summary":"","tags":[],"title":"Asynchrony-Robust Collaborative Perception via Bird's Eye View Flow [NeurIPS2023]","type":"publication"},{"authors":["Chi Xie","Fangao Zeng","Yue Hu","Shuang Liang","Yichen Wei"],"categories":[],"content":"","date":1672531200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":2023,"objectID":"dc4f6647047038b4125b83f7a71aeb78","permalink":"https://yuehu.github.io/publication/xie-hoi-2023/","publishdate":"2023-01-01T00:00:00Z","relpermalink":"/publication/xie-hoi-2023/","section":"publication","summary":"","tags":[],"title":"Category Query Learning for Human-Object Interaction Classification [CVPR2023]","type":"publication"},{"authors":["Yue Hu"],"categories":[],"content":"Where2comm is reported by 机器之心: 《将通信带宽降低至十万分之一，NeurIPS 2022论文提出新一代协作感知方法 》!\n","date":1665584231,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1665584231,"objectID":"4d4801cce70bcdbfb6c65f5bc2c604be","permalink":"https://yuehu.github.io/post/news-where2comm/","publishdate":"2022-10-12T22:17:11+08:00","relpermalink":"/post/news-where2comm/","section":"post","summary":"","tags":["媒体报道","机器之心"],"title":"Where2comm is reported by 机器之心","type":"post"},{"authors":[],"categories":["Recruit"],"content":"一、平台介绍 上海人工智能实验室联合上海交通大学面向海内外公开招聘机器学习、计算机视觉的博士后研究人员若干名，推进群体智能相关领域的研究，助力无人集群、 车路协同、智慧医疗等应用的发展与落地。优厚待遇国际领先。欢迎感兴趣的研究人员应聘。\n上海人工智能实验室是我国人工智能领域的新型科研机构，开展战略性、原创性、前瞻性的科学研究与技术攻关，突破人工智能的重要基础理论和关键核心 技术，打造“突破型、引领型、平台型”一体化的大型综合性研究基地，支撑我 国人工智能产业实现跨越式发展，目标建成国际一流的人工智能实验室，成为享 誉全球的人工智能原创理论和技术的策源地。实验室网址：www.shlab.org.cn\n上海交通大学是我国历史最悠久的高等学府之一，教育部直属、上海市共建、 中央直管的全国重点大学，位列国家“双一流”、“985 工程”、“211 工程”， 为九校联盟、环太平洋大学联盟、21 世纪学术联盟、中国大学校长联谊会、国际 应用科技开发协作网、新工科教育国际联盟成员，入选“珠峰计划”、“强基计划”、“111 计划”、“2011 计划”、卓越工程师教育培养计划等。学校网址： www.sjtu.edu.cn/\n二、岗位职责 研究业界前沿算法，包括但不限于多智能体系统、图机器学习、联邦学习、3D 感知、数据/模型压缩； 负责开源项目的开发和维护，开发新的算法框架，实现 SOTA 算法，提升用户体验和项目影响力； 协助指导博士研究生及实习生的研究工作； 参与申报各类科研项目。 三、申请条件 计算机、人工智能相关专业博士学位；\n深入了解机器学习和计算机视觉的一个或多个方向，以一作身份在顶会或顶刊发表过高质量论文；\n熟练掌握 Python 和 PyTorch，有良好的工程实现能力；\n熟悉 C++、CUDA、ROS，在技术社区有影响力，曾参与贡献知名开源项目者 优先\n四、团队简介 王延峰教授，博士生导师，现任上海人工智能实验室主任助理、全球高校人工智能学术联盟秘书长。国家发改委人工智能专家委员会委员、科技部科技创新 2030“新一代人工智能”重大项目指南专家组成员。主要研究方向为人工智能、 智慧医疗、新兴信息技术商业应用。\n陈思衡，博士生导师，现任上海交通大学电子信息与电气工程学院未来媒体 网络协同创新中心副教授，上海人工智能实验室双聘青年科学家，美国卡内基梅隆大学博士，入选国家级人才计划青年项目。研究方向为群体智能，图机器学习，联邦学习，自动驾驶。\n五、应聘方式 将详细的个人简历，包括教育背景、工作经历、成果情况及联系方式发送至邮箱：sihengc@sjtu.edu.cn ,邮件标题注明“博士后应聘_姓名”。\n","date":1665566531,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1665566531,"objectID":"b7edf4f12f4396b0fe60b57915c6ab1d","permalink":"https://yuehu.github.io/post/recruit-22-fall/","publishdate":"2022-10-12T17:22:11+08:00","relpermalink":"/post/recruit-22-fall/","section":"post","summary":"【长期有效】上海人工智能实验室、上海交通大学招聘博士后","tags":["Recruit","Opening positions"],"title":"Opening positions","type":"post"},{"authors":["Zixing Lei","Shunli Ren","Yue Hu","Wenjun Zhang","Siheng Chen"],"categories":[],"content":"","date":1664582400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1665977596,"objectID":"20ba0c49134e72a041fad8ebbbb5350a","permalink":"https://yuehu.github.io/publication/lei-syncnet-2022/","publishdate":"2022-10-17T03:33:16.597718Z","relpermalink":"/publication/lei-syncnet-2022/","section":"publication","summary":"","tags":[],"title":"Latency-aware collaborative perception [ECCV2022]","type":"publication"},{"authors":null,"categories":null,"content":"\r[Sep. 2025] BeliefMapNav and NegoCollab is accepted by NeurIPS 2025\n[Aug. 2025] SGImagineNav is accepted by ICCV Demo Track 2025\n[Apr. 2025] V2XVerse is accepted by TPAMI 2025\n[Mar. 2025] MAS workshop proposal is accepted by ICML 2025\n[Feb. 2025] EvoMAC is accepted by ICLR 2025\n[Feb. 2024] CodeFilling is accepted by CVPR 2024\n[Jan. 2024] HEAL is accepted by ICLR 2024\n[Nov. 2023] CoBEVFlow is accepted by NeurIPS 2023\n[Apr. 2023] CoCa3D is reported by 机器之心\n[Mar. 2023] Two papers are accepted to CVPR 2023\n[Jan. 2023] Aerial Monocular 3D Object Detection is accepted to IEEE Robotics and Automation Letters\n[Oct. 2022] Where2comm is reported by 机器之心 and 将门创投\n[Sep. 2022] One paper is accepted to NeurIPS 2022 Spotlight(Top5%)\n","date":1664582400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1664582400,"objectID":"a0812ae5f3c926fea6faf4472cefc8e2","permalink":"https://yuehu.github.io/news/","publishdate":"2022-10-01T00:00:00Z","relpermalink":"/news/","section":"","summary":"List of news.\r\n","tags":[],"title":"News","type":"page"},{"authors":["Yue Hu","Shaoheng Fang","Zixing Lei","Yiqi Zhong","Siheng Chen"],"categories":[],"content":"","date":1663200000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1665977597,"objectID":"0112e2bf292b7ae36081aab77408f743","permalink":"https://yuehu.github.io/publication/hu-where-2-comm-2022/","publishdate":"2022-10-17T03:33:17.569007Z","relpermalink":"/publication/hu-where-2-comm-2022/","section":"publication","summary":"","tags":[],"title":"Where2comm: Communication-efficient collaborative perception via spatial confidence maps [NeurIPS2022 Spotlight]","type":"publication"},{"authors":[],"categories":["Recruit"],"content":"MediaBrain陈思衡老师 招【2023/24级】直博/直硕生啦！\n陈思衡老师是未来媒体网络协同创新中心的长聘轨副教授，2019年国家重大人才工程青年项目入选者！ 陈老师在CMU获得博士学位，博士后完成后，在Uber Advanced Technologies Group、三菱电机实验室MERL担任Research Scientist，至今在TPAMI、TIP、NeurIPS（oral）、CVPR （oral）、AAAI （oral）、ICLR上发表了50余篇论文，Google Scholar引用2000余次，获得过IEEE信号处理协会最佳年轻作者论文奖！学界+业界影响力Max！\n陈老师与海外各高校包括（CMU、Oxford、…）以及国内外各企业（Uber、滴滴、…）都有着很强的connection，可以推荐你到世界顶尖的平台学习交流！\n如果你数学与编程功底扎实，踏实好学，对计算机视觉与机器学习有一定的了解； 如果你想做【一流的学术】工作、在国际顶级期刊和会议发表【高水平 \u0026amp; 有学术影响力】的论文； 如果你对图信号处理、图神经网络、无人系统等领域感兴趣，想要探索计算机视觉、机器学习和自动驾驶的问题； 如果对科研有热情，希望能够加入一个积极向上、开放包容、有激情有理想地探索科学的、顶级的科研氛围与团队； 那么欢迎你加入我们！ 加入我们，你会接受系统的科研工作练习，陈老师与学长学姐带你快速适应科研节奏，参与到我们正在开展的工作中，有机会在一年做出优质的科研工作并发表顶会论文！\n请有意向的同学发送简历至：sihengc[AT]sjtu[DOT]edu[DOT]cn\n也欢迎朋友们帮忙转发，期待大家共同进步！\n","date":1651665208,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1651665208,"objectID":"a88b1de987243c1ec7a1d581b409cc8e","permalink":"https://yuehu.github.io/post/recruit-22-spring/","publishdate":"2022-05-04T19:53:28+08:00","relpermalink":"/post/recruit-22-spring/","section":"post","summary":"2023/2024级直博/硕士生招生","tags":["Recruit","Opening positions"],"title":"We are hiring !","type":"post"},{"authors":["Shaoheng Fang","Yue Hu","Weitao Wu","Siheng Chen"],"categories":[],"content":" Table of Contents About Coperception-UAV Dataset Simulation Setting Swarm arrangement Sensor Setup Data Usage Citation About Coperception-UAV Dataset Coperception-UAV is the first comprehensive dataset for UAV-based collaborative perception.\nA UAV swarm has the potential to distribute tasks and achieve better, faster, and more robust performances than a single UAV. To realize this, we need to integrate collaboration ability into the entire pipeline, including perception, planning, control. Among those tasks, collaborative perception enables holistic scene understanding from multiple perspectives via the collaboration of multiple UAVs, which could fundamentally resolve the occlusion issue and the long-range issue in the traditional single-agent perception. Recently, planning and control of a UAV swarm have been intensively studied; however, the collaborative perception remains under-explored due to the lack of a comprehensive dataset. This work aims to fill this gap and proposes a collaborative perception dataset for UAV swarm.\nBased on the co-simulation platform of AirSim and CARLA, our dataset consists of 131.9k synchronous images collected from 5 coordinated UAVs flying at 3 altitudes over 3 simulated towns with 2 swarm formations. Each image is fully annotated with the pixel-wise semantic segmentation labels and 2D/3D bounding boxes of vehicles. We further build a benchmark on the proposed dataset by evaluating a variety of related multi-agent collaborative methods on multiple perception tasks, including object detection, semantic segmentation, and bird’seye-view (BEV) semantic segmentation.\nSimulation Setting Our proposed dataset is collected by the co-simulation of CARLA and AirSim. We use CARLA to generate complex simulation scenes and traffic flow; and use AirSim to simulate UAV swarm flying in the scene. The flight route of UAVs is controlled by AirSim and sample data are collected randomly at about 4-second intervals.\nSwarm arrangement The UAV swarm moves and executes tasks in the three-dimensional space, where the situation could be much more complex than those of vehicles or roadside units. In the dataset, two main factors are taken into consideration that may affect the perception and collaboration patterns of UAV swarms: flight formation and altitude. Each UAV swarm consists of 5 UAVs. We arrange two types of formation modes for a UAV swarm: discipline mode, where all 5 UAVs keeps a consistent and relatively static array, and dynamic mode, where each UAV navigates independently in the scene. The former simulates the situation where the swarm of UAVs is executing a same specific task such as exploring an unknown area, search and rescue; while the latter simulates the monitoring and patrolling tasks in the city.\nSensor Setup In the UAV swarm, Each UAV is equipped with 5 RGB cameras in 5 directions and 5 semantic cameras collecting semantic ground truth for RGB cameras.\n90° horizontal FoV 1 bird’s eye view camera and 4 cameras facing forward, backward, right, and left with a pitch degree of −45° image size: 800x450 pixels Data Fully-annotated data are provided in the dataset, including synchronous images with pixel-wise semantic labels, 2D \u0026amp; 3D bounding boxes of vehicles, and BEV semantic map.\nCamera data We collect synchronous images from all cameras on 5 UAVs, which is 25 images in a sample. In total, 123.8K images are collected for the discipline swarm mode and 8.1K for the dynamic swarm mode. We provide semantic label for each image.\nBounding boxes 3D bounding boxes of vehicles are recorded at the same moment with images, including location (x, y, z), rotation (w, x, y, z in quaternion) in the global coordinate and their length, width and height. To specifically address the occlusion issue, we also provide a binary label for the occlusion status of each bounding box.\nBEV semantic label We provide BEV segmentation labels of four categories: roadway, building, vehicle, and others, which are the key elements to construct the layout of a city and foreground objects. The resolution of the BEV map is 0.25m×0.25m.\nUsage The dataset is organized in a similar way with the widelyused autonomous driving dataset, nuScenes; so it can be used directly with the well-established nuScenes-devkit.\nCitation To Be Done ","date":1650370064,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1650370064,"objectID":"97c4d20f8094ee47b07469162a696c84","permalink":"https://yuehu.github.io/dataset/coperception-uav/","publishdate":"2022-04-19T20:07:44+08:00","relpermalink":"/dataset/coperception-uav/","section":"dataset","summary":"Coperception-UAV is the first comprehensive dataset for UAV-based collaborative perception.","tags":[],"title":"Coperception-UAV Dataset","type":"dataset"},{"authors":["Cheng Zou","Bohan Wang","Yue Hu","Junqi Liu","Qian Wu","Yu Zhao","Boxun Li","Chenguang Zhang","Chi Zhang","Yichen Wei","Jian Sun"],"categories":[],"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1666084476,"objectID":"015c45e0372b7c31adb6ee61af7953e5","permalink":"https://yuehu.github.io/publication/zou-hoi-2021/","publishdate":"2021-01-01T00:00:00Z","relpermalink":"/publication/zou-hoi-2021/","section":"publication","summary":"","tags":[],"title":"End-to-end human object interaction detection with hoi transformer [CVPR2021]","type":"publication"},{"authors":["Yue Hu","Siheng Chen","Ya Zhang","Xiao Gu"],"categories":[],"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1666084476,"objectID":"082c6895f7a0644fc8a573470a2faa94","permalink":"https://yuehu.github.io/publication/hu-nmmp-2020/","publishdate":"2022-10-18T09:14:36.141644Z","relpermalink":"/publication/hu-nmmp-2020/","section":"publication","summary":"","tags":[],"title":"Collaborative motion prediction via neural motion message passing [CVPR2020 Oral]","type":"publication"},{"authors":["Yue Hu","Siheng Chen","Xu Chen","Ya Zhang","Xiao Gu"],"categories":[],"content":"","date":1559347200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559347200,"objectID":"4215819419b1beb6c547fbc1c7e7c3ad","permalink":"https://yuehu.github.io/publication/hu-vrd-2019/","publishdate":"2019-06-01T00:00:00Z","relpermalink":"/publication/hu-vrd-2019/","section":"publication","summary":"","tags":[],"title":"Neural message passing for visual relationship detection [ICML2019 Workshop Spotlight]","type":"publication"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Wowchemy Wowchemy | Documentation\nFeatures Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides Controls Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026#34;blueberry\u0026#34; if porridge == \u0026#34;blueberry\u0026#34;: print(\u0026#34;Eating...\u0026#34;) Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}} Press Space to play!\nOne Two Three A fragment can accept two optional parameters:\nclass: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}} Press the S key to view the speaker notes!\nOnly the speaker can read these notes Press S key to view Themes black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026#34;/media/boards.jpg\u0026#34; \u0026gt;}} {{\u0026lt; slide background-color=\u0026#34;#0000FF\u0026#34; \u0026gt;}} {{\u0026lt; slide class=\u0026#34;my-style\u0026#34; \u0026gt;}} Custom CSS Example Let’s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; } Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://yuehu.github.io/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Wowchemy's Slides feature.","tags":[],"title":"Slides","type":"slides"}]